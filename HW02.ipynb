{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 2\n",
    "\n",
    "Deadline: March 10th 11:59pm\n",
    "\n",
    "Hand in: the homework must be handed in through the Moodle system. \n",
    "\n",
    "<span style=\"color:red\">Provide a written answer if requested in the exercise! These questions are marked in red.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs are a great way to collect data for your projects. Here are a few API you can try out:\n",
    "\n",
    "- https://developer.nytimes.com/docs/archive-product/1/overview\n",
    "- https://api.wikimedia.org/wiki/Getting_started_with_Wikimedia_APIs\n",
    "- https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_get_paper_citations\n",
    "\n",
    "In this exercise you'll collect and clean data from an API. This could be a good oportunity to collect data for your final project. (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Collect data from an API of your choice. Proccess the returned data so that each unique data feature is a column in a pandas dataframe. Investigate missing data, such as NaN values, and apply a solution. Cast data columns to be the right type for the data they contain. Display the cleaned data frame using .head()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'lt06iVOvngWYAy29i79LsYG8fa99LrVn' \n",
    "url = 'https://api.nytimes.com/svc/archive/v1/2019/1.json?api-key=' + api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url).json()['response']['docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./Data/nytimes.json\", \"w\") as file:\n",
    "    json.dump(response, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2 = pd.DataFrame(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that print_section and print_page columns have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df2))\n",
    "print(len(df2['print_page'].dropna()))\n",
    "print(len(df2['print_section'].dropna()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter by important columns (and columns that don't have missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"id\", \"abstract\", \"section_name\", \"headline\", \"pub_date\", \"document_type\", \"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id, article in enumerate(response):\n",
    "    df_row = { \n",
    "        \"id\": id,\n",
    "        \"abstract\": article[\"abstract\"], \n",
    "        \"section_name\": article[\"section_name\"], \n",
    "        \"headline\": article[\"headline\"][\"print_headline\"], \n",
    "        \"pub_date\": datetime.strptime(article[\"pub_date\"], \"%Y-%m-%dT%H:%M:%S%z\"), \n",
    "        \"document_type\": article[\"document_type\"], \n",
    "        \"word_count\": int(article[\"word_count\"])\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame(df_row, index=[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Use the dataframe created in part a) to answer an exploratory data analysis question of your choice. State your question, design a data visualization that answers your question and <span style=\"color:red\">discuss</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot how many articles were published for each section weekly. Filter by top 10 topics for each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([df['pub_date'].dt.strftime('%W'),'section_name']).size().unstack().apply(lambda x: x.nlargest(10), axis=1).plot(\n",
    "    kind='bar',\n",
    "    stacked=True, \n",
    "    title='Number of articles published per week',\n",
    "    xlabel='Calendar Week in 2019',\n",
    "    ylabel='Number of articles',\n",
    "    figsize=(12,8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Round University Ranking (RUR)](https://roundranking.com/ranking/world-university-rankings.html#world-2021) evaluates the performance of 867 worldâ€™s leading higher education institutions by 20 indicators grouped into 4 key areas of university activity: Teaching, Research, International Diversity, Financial Sustainability. The top 100 universities are placed in the diamond league, the next 100 in the gold league and so on... (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)**  Using the scraping techniques covered in class, scrape the following data fields about the universities (from the website linked above): The name of the University, in which country the University is located, their score and league given by the RUR ranking. Then load the data into a Pandas DataFrame called *df* with the following column names: <font style='font-style : oblique'>University</font>, <font style='font-style : oblique'>Country</font>, <font style='font-style : oblique'>Score</font> and <font style='font-style : oblique'>League</font>. \n",
    "\n",
    "IMPORTANT: You should not re-scrape the data every time you work on the homework, because we don't want the RUR servers to get overloaded. Instead, scrape the data once and then save it to a local file on your computer (Hint: use the *pd.to_csv()* function), then load the data from this file instead of re-scraping the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nia/.local/lib/python3.10/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/nia/.local/lib/python3.10/site-packages (from beautifulsoup4) (2.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in /home/nia/.local/lib/python3.10/site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in /home/nia/.local/lib/python3.10/site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /home/nia/.local/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/nia/.local/lib/python3.10/site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: trio~=0.17 in /home/nia/.local/lib/python3.10/site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /home/nia/.local/lib/python3.10/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: sortedcontainers in /home/nia/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /home/nia/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in /home/nia/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /home/nia/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: idna in /home/nia/.local/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: wsproto>=0.14 in /home/nia/.local/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /home/nia/.local/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /home/nia/.local/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: webdriver-manager in /home/nia/.local/lib/python3.10/site-packages (4.0.1)\n",
      "Requirement already satisfied: packaging in /home/nia/.local/lib/python3.10/site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: requests in /home/nia/.local/lib/python3.10/site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /home/nia/.local/lib/python3.10/site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nia/.local/lib/python3.10/site-packages (from requests->webdriver-manager) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nia/.local/lib/python3.10/site-packages (from requests->webdriver-manager) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nia/.local/lib/python3.10/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nia/.local/lib/python3.10/site-packages (from requests->webdriver-manager) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip3 install selenium\n",
    "!pip3 install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get('https://roundranking.com/ranking/world-university-rankings.html#world-2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(driver.page_source, 'html.parser')\n",
    "table = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we don't have to rescrape the website by saving the table as an html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/soup.html', 'w') as file:\n",
    "    file.write(table[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the table into a dataframe and clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find('table')\n",
    "table_rows = table.find_all('tr')\n",
    "data = []\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [tr.text for tr in td]\n",
    "    data.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=['Rank', 'University', 'Score', 'Country', 'Flag', 'League', 'Continent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.index[0])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.drop(columns=['Rank', 'Flag', 'Continent'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Data/university_rankings.csv', 'w') as file:\n",
    "    df.to_csv(file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Filter the data as follows:\n",
    "- Filter out the US universities. (The analysis aims to find out which universities rank high outside the USA to help US students in choosing a study abroad program.)\n",
    "- Only keep the universities in the Diamond, Gold, Silver and Bronze league.\n",
    "- Sort the dataframe by score. <span style=\"color:red\">Which are the top 5 ranking universities?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "with open('./Data/university_rankings.csv', 'r') as file:\n",
    "    df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_abroad_unis = df[\n",
    "    (df[\"Country\"] != \"USA\") & \n",
    "    df[\"League\"].isin([\n",
    "        \"Diamond League\",\n",
    "        \"Golden League\", \n",
    "        \"Silver League\", \n",
    "        \"Bronze League\"\n",
    "        ])\n",
    "    ]\n",
    "study_abroad_unis = study_abroad_unis.sort_values(by=\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          University   Score      Country  \\\n",
      "3                            Imperial College London  96.742           UK   \n",
      "4                               Karolinska Institute  96.609       Sweden   \n",
      "6                               University of Oxford  96.167           UK   \n",
      "7  ETH Zurich (Swiss Federal Institute of Technol...  94.629  Switzerland   \n",
      "8                            University of Cambridge  94.383           UK   \n",
      "\n",
      "           League  \n",
      "3  Diamond League  \n",
      "4  Diamond League  \n",
      "6  Diamond League  \n",
      "7  Diamond League  \n",
      "8  Diamond League  \n"
     ]
    }
   ],
   "source": [
    "print(study_abroad_unis.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 ranking universities outside US are the following\n",
    "\n",
    "- Imperial College London\n",
    "- Karolinska Institute\n",
    "- University of Oxford\n",
    "- ETH Zurich\n",
    "- University of Cambridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Create a word cloud from the Mission Statements of the top Universities. We have already scraped these statements for you. You can find the scraped data [here](https://math.bme.hu/~pinterj/BevAdat1/Adatok/wordcloud.txt)! <br>\n",
    "- Load the text data from this site into a string variable! (Hint: You can load the data with *urlopen* as shown in Notebook2)\n",
    "- Omit the word university from the data!\n",
    "- Create a word cloud, then <span style=\"color:red\">describe what you see in 2-3 sentences!</span>\n",
    "\n",
    "(Hint: You can find more information on how to create a Word Cloud at https://www.datacamp.com/community/tutorials/wordcloud-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://math.bme.hu/~pinterj/BevAdat1/Adatok/wordcloud.txt\"\n",
    "source = urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = str.replace(source, \"university\", \"\")\n",
    "source = str.replace(source, \"University\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve on the model used in class: decision tree on the bank dataset. (40%) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** In class we solved a classification problem on the *bank.csv* dataset using the kNN algorithm. The classifier performed poorly. Repeat the analysis carried out in class (based on the Notebook02), but now use a decision tree, set the maximum depth to be 6! **Hint:** Use the *tree.DecisionTreeClassifier* classifier!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Now fit the tree using different parameters! Plot the ROC curve of the decision tree obtained in part a) and the new tree in the same figure (with different colors). Also plot the *y=x* diagonal line!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Plot and interpret the decision tree. The easiest way to do this is with the sklearn.tree.plot_tree function. Here's a useful article: https://pythoninoffice.com/how-to-a-plot-decision-tree-in-python/.\n",
    "\n",
    "* If you would like, you can try plotting the decision tree using the graphviz package too. **Hints:** Visualize the decision tree trained in part a) using the *tree.export_graphviz* function. To present the tree use the *graphviz.Source* function or the *SVG* function of the *Ipython.display* package! If *graphviz* is not installed you can install it using the Anaconda Navigator or by using *pip install* or by installing with homebrew, *brew install graphviz*. If it doesn't seem to work you can also download it from this [link](https://graphviz.gitlab.io/download/) and inserting the following lines of codes (use the correct path for your downloaded file):<br><br>\n",
    "import os <br>\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin') <br><br>\n",
    "\n",
    "* <span style=\"color:red\">Briefly interpret the results! According to what attribute did we split the tree first? Which were the usual splitting attributes? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Plot the feature importances for the decision tree. The link in part c) also has useful information about this. <span style=\"color:red\"> Briefly interpret. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works Cited:\n",
    "\n",
    "Please cite all external resources you used to complete this assignment. If you used ChatGPT, please include a link to the conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(https://stackoverflow.com/questions/45281297/group-by-week-in-pandas)[df]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a [adding new rows to df](https://stackoverflow.com/questions/75956209/error-dataframe-object-has-no-attribute-append)\n",
    "\n",
    "1.b [grouping dates by week](https://stackoverflow.com/questions/45281297/group-by-week-in-pandas)\n",
    "\n",
    "[pandas cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf)\n",
    "\n",
    "2.b [filtering columns by multiple conditions](https://chat.openai.com/share/ad7425b9-8f06-4c33-9922-02739ea46cd8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
